
# AWS Data Ingestion Project

This project implements a simple architecture to ingest data in AWS using Terraform, Lambda Functions, and PostgreSQL.

## Project Overview

The main objective is to create an AWS infrastructure using Terraform, which will deploy:
- A VPC with subnets.
- An Internet Gateway for external access.
- A PostgreSQL RDS instance.
- A security group to allow access from a specific IP.
- An S3 bucket for storing data.
- A set of Python scripts to upload data to S3, create tables in the database, and load data into those tables.
  
The project structure includes Terraform code for provisioning the resources, Python scripts for data handling, and sample data stored in CSV format.

## Project Structure

```
├── dbml/
│   └── tables_diagram_old.txt   # DB original schema file in DBML format
│   └── tables_diagram.txt   # DB modified schema file in DBML format
├── logs/
│   └── *.log                # Logs generated by the Python scripts
├── sample_data/
│   ├── table_name_1/
│   │   └── data.csv         # CSV files to load into the database
│   ├── table_name_2/
│   │   └── data.csv
│   └── README               # Readme explaining folder structure and data
├── scripts/
│   ├── upload_to_s3.py      # Script to upload data from local to S3
│   ├── create_tables.py     # Creates PostgreSQL tables based on DBML file
│   ├── load_data_bulk.py    # Loads data from CSVs into PostgreSQL
│   ├── create_constraints.py# Creates constraints in PostgreSQL
├── terraform/
│   ├── main.tf              # Main Terraform file for resource creation
│   ├── outputs.tf           # Outputs to be used after provisioning
│   ├── terraform.tfvars     # Terraform variables configuration
│   └── variables.tf         # Defines variables used in Terraform
├── .env                     # Environment variables used in Terraform and Python scripts
├── .gitattributes           # File used to define attributes for paths in the repository (Allows dbml diagram not to be modified)
├── .gitignore               # File that tells Git which files or directories to ignore and not track in version control
├── requirements.txt         # Dependencies for Python scripts
└── README.md                # Project overview and setup instructions
└── run_pipelines.py     # Main script to execute all processes
```

## Requirements

- Terraform installed (`>=1.0`)
- AWS CLI configured
- Python 3.9 or higher
- Access to an AWS account (preferably with free tier)
- A valid PostgreSQL DB username, password, and name for the database

## Steps to Run

1. Clone this repository:

   ```bash
   git clone https://github.com/your-repository/aws-data-ingestion.git
   cd aws-data-ingestion
   ```

2. Set up your `.env` file by executing Terraform:

   ```bash
   cd terraform
   terraform init
   terraform apply
   ```

   **It should be executed into terraform/ folder**

3. Run the pipeline script in your main folder:

   ```bash
   python run_pipelines.py
   ```

   - The pipeline can run the following scripts:
     - `upload_to_s3.py`: Uploads CSV files to S3.
     - `create_tables.py`: Creates tables in PostgreSQL RDS based on the DBML file.
     - `load_data_bulk.py`: Loads CSV data into PostgreSQL tables.
     - `create_constraints.py`: Adds constraints to the tables based on DBML.


4. Logs of the execution will be saved in the `logs/` folder.

## Python Scripts

1. **upload_to_s3.py**: This script uploads local CSV files into the designated S3 bucket.
2. **create_tables.py**: It creates tables in the PostgreSQL database using the DBML file. It will recreate the tables if they already exist.
3. **load_data_bulk.py**: This script uses `copy_expert` to load data from CSV files into the PostgreSQL tables. It also creates dummy records for time dimension tables (d_time, d_week, etc.).
4. **create_constraints.py**: Adds constraints (primary keys, foreign keys, etc.) to the PostgreSQL tables based on the DBML schema.

## Terraform Configuration

The Terraform files will provision:

1. A VPC with two subnets in different availability zones.
2. An internet gateway for public internet access.
3. A PostgreSQL RDS instance with specific configurations.
4. A security group allowing access to PostgreSQL only from your IP.
5. An S3 bucket for storing data.

The configuration is modular and reusable across different environments by modifying the `terraform.tfvars` file.

## Environment Variables

During the execution of Terraform, the `.env` file will be generated automatically. This file contains important variables such as:

- `BUCKET_NAME`: The name of the S3 bucket.
- `DB_HOST`: The host address of the PostgreSQL RDS instance.
- `DB_NAME`: The name of the PostgreSQL database.
- `DB_USER`: The PostgreSQL username.
- `DB_PASSWORD`: The PostgreSQL password.
- `DB_PORT`: The PostgreSQL port (default: 5432).

These environment variables are used both in the Terraform configuration and the Python scripts.

## Running the Pipeline

To run the full data ingestion pipeline, you can use the `run_pipelines.py` script. It will execute each of the following in sequence:

1. **Upload data to S3 (optional)**: The `--upload` argument uploads the CSV data to S3.
2. **Create PostgreSQL tables**: Using the DBML schema, the tables are created in the RDS database.
3. **Load data into PostgreSQL**: Data is loaded from the CSV files into the corresponding PostgreSQL tables.
4. **Create constraints**: Finally, the constraints are created for the tables.

Logs for each step are saved in the `logs/` folder for troubleshooting and auditing.

## Conclusion

This project allows you to automate the setup of an AWS environment for data ingestion, load data into PostgreSQL, and create the necessary database schema. It can be further extended to support more complex ETL workflows.
